# -*- coding: utf-8 -*-
"""Machine Failure Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bUEvPL2UTiMXI4zT10B94_QYD_P08B0m
"""

# importing drive to colab
from google.colab import drive
drive.mount("/content/gdrive")
print("done")

# importing requirements
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

print('Done')

# load the data
df_train = pd.read_csv("/content/gdrive/MyDrive/Machine_Failure_Prediction/train.csv")
df_test = pd.read_csv("/content/gdrive/MyDrive/Machine_Failure_Prediction/test.csv")
df_original = pd.read_csv("/content/gdrive/MyDrive/Machine_Failure_Prediction/machine_failure.csv")

print("done")

df_train.head()

"""Column names contains ],[,white spaces"""

def clean_col_names(df):
  df.columns = df.columns.str.replace('[', '')
  df.columns = df.columns.str.replace(']', '')
  df.columns = df.columns.str.replace(' ', '_')
  df.columns = df.columns.str.replace(':', '')
  return df

df_train = clean_col_names(df_train)
df_test = clean_col_names(df_test)
df_original = clean_col_names(df_original)

df_train.head()

# df_test.head()

# df_original.head()

# # df_train.info()
# df_test.info()
# df_original.info()

# df_train.describe()

# df_test.describe()

# df_original.describe()

df_train.columns

df_train.drop(['id'], axis=1, inplace=True)
df_test.drop(['id'], axis=1, inplace=True)
df_original.drop(['UDI'], axis=1, inplace=True)

df_train.shape, df_test.shape, df_original.shape

# df_train['Product_ID'].head()

# Transforming Product_ID

dfs = [df_train, df_test, df_original]

for df in dfs:
  df['Product_ID'] = df['Product_ID'].str[1:].astype(int)


print("Removed the first letter from Product_ID")

df_train.head()

df_train = pd.concat([df_train, pd.get_dummies(df_train['Type'], prefix='Type').astype(int)], axis=1)
df_train.drop(['Type'], axis=1, inplace=True)


df_test = pd.concat([df_test, pd.get_dummies(df_test['Type'], prefix='Type').astype(int)], axis=1)
df_test.drop(['Type'], axis=1, inplace=True)

df_original = pd.concat([df_original, pd.get_dummies(df_original['Type'], prefix='Type').astype(int)], axis=1)
df_original.drop(['Type'], axis=1, inplace=True)

print("Encoding the Type column -- Done")

df_train.head()

df_train.iloc[:,1:6].plot(kind='density', subplots=True, layout=(3,3), sharex=False, sharey=False)
plt.rcParams['figure.figsize'] = (14,14)
plt.show()
print('Density plot done')

df_train.isna().sum()

df_train.duplicated().sum()

df_train = df_train.drop_duplicates()

df_test = df_test.drop_duplicates()

df_original.duplicated().sum()

df_train.nunique().sort_values(ascending=False)

"""Concating df_train and df_original"""

df_train.shape, df_original.shape

df_total = pd.concat([df_train, df_original], axis=0)
df_total.shape



"""Feature Engineering"""

train_fe = df_train.copy()
test_fe = df_test.copy()
total_fe = df_total.copy()

def feature_engineering(df):
  df['RelationTemperature'] = df['Air_temperature_K']/df['Process_temperature_K']  # how much hotter the process is compared to the environment.
  df['TorquesRPM'] = df['Torque_Nm']/df['Rotational_speed_rpm']  # Measures torque applied per unit of rotation.
  df['WearRPM'] = df['Tool_wear_min']/df['Rotational_speed_rpm'] # How much wear the tool experiences per unit speed.
  df['ToolWearTorque'] = df['Tool_wear_min']/df['Torque_Nm'] # High value = tool degrades faster for given torque → poor efficiency or tool quality.
  df['Stress_Index'] = df['Torque_Nm'] * df['Rotational_speed_rpm'] / (df['Tool_wear_min'] + 1) # High index = machine is under mechanical stress, especially when tool wear is still low
  df['Delta_temperature'] = df['Process_temperature_K'] - df['Air_temperature_K'] # too high → overheating risk


  for col in ['Torque_Nm', 'Tool_wear_min', 'Rotational_speed_rpm']:
    df[f'{col}_B'] = pd.cut(df[col], bins=20)
    df[f'{col}_B'] = df[f'{col}_B'].cat.codes + 1

  df['TotalF'] = df['TWF'] + df['HDF'] + df['PWF'] + df['OSF'] # Higher value means multiple failure mechanisms are active → more severe case.


  Angular_velocity = df['Rotational_speed_rpm']* 2 * np.pi / 60 # Converting RPMs to radians per second (true rotational velocity)

  df['Power_W'] = df['Torque_Nm'] * Angular_velocity # instantaneous workload on machine

  df['Energy_J'] = df['Power_W'] * df['Tool_wear_min'] * 60 # High energy usage + high wear = poor efficiency.

  return df

train_fe = feature_engineering(train_fe)
test_fe = feature_engineering(test_fe)
total_fe = feature_engineering(total_fe)

total_fe.shape, total_fe.columns, test_fe.shape, test_fe.columns

target = 'Machine_failure'

corr_with_target = train_fe.corr()[target].sort_values(ascending=False)

print("Correlation of features with target column:\n")
print(corr_with_target)

"""Correlation only captures linear relationship, for better approach inspecting for non-linear relationships using mutual information., So, before finalizing which features to use we are using Ensemble(Random Forest Classifier) methods to find most relevant features to take for training the model."""

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import mutual_info_classif
import pandas as pd

x1 = train_fe.drop(columns=[target, 'Product_ID', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF', 'TotalF'], axis = 1) # dropping irrelevant columns and ones that may leak the answer
y1 = train_fe[target]

# Step 1: Compute Mutual Information (MI)
mi_scores = pd.Series(mutual_info_classif(x1, y1, random_state=24),
                      index=x1.columns).sort_values(ascending=False)

# Select top N features based on MI
top_n = 20   # you can tune this (10–15 is good)
selected_features = mi_scores.head(top_n).index.tolist()

print(f"Top {top_n} features by Mutual Information:\n")
print(mi_scores.head(top_n))
print("\n")

# Step 2: Train Random Forest using only these features
rf = RandomForestClassifier(n_estimators=200, random_state=24)
rf.fit(x1[selected_features], y1)

# Step 3: Random Forest Feature Importances
rf_importances = pd.Series(rf.feature_importances_,
                           index=selected_features).sort_values(ascending=False)

print("Random Forest Feature Importances:\n")
print(rf_importances)

# Step 4: Side-by-side comparison (MI vs RF)
comparison = pd.DataFrame({
    "Mutual Information": mi_scores[selected_features],
    "RandomForest Importance": rf_importances
}).sort_values("RandomForest Importance", ascending=False)

print("\nComparison of MI vs RF Importances:\n")
print(comparison)

"""Checking the class imbalance"""

class_counts = total_fe[target].value_counts()
class_percentages = total_fe[target].value_counts(normalize=True)*100

print("Class distribution\n")
print(pd.DataFrame({'Count':class_counts,
                   'Percent':class_percentages.round(2)}))

!pip install catboost

from sklearn.model_selection import train_test_split
from sklearn.metrics import *
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

x = total_fe.drop(columns=[target, 'Product_ID', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF', 'TotalF'])
y = total_fe[target]

x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=0.3, stratify=y, random_state=35)

scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train)  # imbalance weight for XGBoost

models = {
    'RandomForest': RandomForestClassifier(
        n_estimators=200, class_weight='balanced', random_state=35
    ),
    'XGBoost': XGBClassifier(
        n_estimators=300, learning_rate=0.05, max_depth=6, scale_pos_weight=scale_pos_weight, eval_metric='logloss', random_state=35
    ),
    'LightGBM':LGBMClassifier(
        n_estimators=300, learning_rate=0.05, class_weight='balanced', random_state=35
    ),
    'CatBoost':CatBoostClassifier(
        iterations=300, learning_rate=0.05, depth=6, auto_class_weights='Balanced', verbose=0, random_state=35
    )
}

results= []

for name, model in models.items():
  print(f"Training {name}")
  model.fit(x_train, y_train)

  y_pred=model.predict(x_val)
  y_proba=model.predict_proba(x_val)[:,1]

  roc_auc = roc_auc_score(y_val, y_proba)

  print("ROC-AUC:", round(roc_auc, 4))
  print("Confusion matrix:", confusion_matrix(y_val, y_pred))
  print("Classification Report:", classification_report(y_val, y_pred))

  results.append([name, roc_auc])

results_df = pd.DataFrame(results, columns=['Model', 'ROC-AUC'])
print("Model Comparison:", results_df.sort_values(by='ROC-AUC', ascending=False))

"""ROC-AUC represents the ability to seperate the classes.ROC-AUC > 0.9, model is very good. SO CatBoost wins here.

Using hyper-parameter tuning to enhance performance of top 2 models (i.e CatBoost and LightBGM)

Recall: Out of all actual failures, how many did model predicted correctly?
Precision: Out of all predicted failures, how many were correct?
Accuracy: Out of all machines(failed & non-failed), what fraction did model predicted correctly?
"""

from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=35)

cat = CatBoostClassifier(
    verbose=0,
    random_state=35,
    auto_class_weights='Balanced'
)

cat_params = {
    'depth':[4,6,8,10],
    'learning_rate':[0.01,0.05,0.1],
    'iterations':[200,500,800],
    'l2_leaf_reg':[1,3,5,7]
}

cat_search = RandomizedSearchCV(
    estimator=cat,
    param_distributions=cat_params,
    n_iter=10,
    scoring='recall',
    cv=cv,
    n_jobs=-1,
    random_state=35,
    verbose=1
)

cat_search.fit(x,y)
print("Best CatBoost params:", cat_search.best_params_)
print("Best CatBoost CV Recall:", cat_search.best_score_)

lgb = LGBMClassifier(
    random_state=42,
    class_weight='balanced'
)

lgb_params = {
    'num_leaves':[31,63,127],
    'max_depth':[-1, 6, 10],
    'learning_rate':[0.01, 0.05, 0.1],
    'n_estimators':[200, 500, 800],
    'subsample':[0.7, 0.8, 1.0],
    'colsample_bytree':[0.7, 0.8, 1.0]
}

lgb_search = RandomizedSearchCV(
    estimator = lgb,
    param_distributions = lgb_params,
    n_iter=10,
    scoring='recall',
    cv=cv,
    n_jobs=-1,
    random_state = 35,
    verbose=1
)

lgb_search.fit(x,y)
print("Best LightGBM params:", lgb_search.best_params_)
print("Best LightGBM CV Recall:", lgb_search.best_score_)

print("Best LightGBM CV Recall:", lgb_search.best_score_)
print("Best CatBoost CV Recall:", cat_search.best_score_)

"""Recall is our main metric and CatBoost is giving highest recall, so picking it."""

best_cat = CatBoostClassifier(
    iterations=200, learning_rate=0.05, depth=4, auto_class_weights='Balanced', verbose=0, random_state=35, l2_leaf_reg=7   # retrainig model with best params
)

best_cat.fit(x,y)

test_preds_proba = best_cat.predict_proba(test_fe)[:,1]   # training on test set
test_preds = (test_preds_proba > 0.5).astype(int)

# Finding optimum threshold
y_proba = best_cat.predict_proba(x)[:,1]

thresholds = np.arange(0.1,1.0,0.1)
results = []

for t in thresholds:
  y_pred = (y_proba >= t).astype(int)

  precision = precision_score(y, y_pred)
  recall = recall_score(y, y_pred)
  f1 = f1_score(y, y_pred)
  accuracy = accuracy_score(y, y_pred)

  results.append([t, precision, recall, f1, accuracy])

df_thresholds = pd.DataFrame(results, columns=['Threshold', 'Precision', 'Recall', 'F1', 'Accuracy'])
print(df_thresholds)



